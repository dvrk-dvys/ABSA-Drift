# TikTok Comment Sentiment Analysis - MLOps Project Structure
# HOURLY Batch Processing with EventBridge + Step Functions + Lambda

================================================================================
## DIRECTORY STRUCTURE
================================================================================

ABSA-Drift/
├── .github/
│   └── workflows/
│       ├── ci.yml                          # Unit tests, linting, building containers
│       └── cd.yml                          # Deploy to staging/prod environments
├── src/
│   ├── lambdas/
│   │   ├── extract/
│   │   │   ├── Dockerfile                  # Container for lambda-extract
│   │   │   ├── requirements.txt            # Pandas, boto3, s3fs
│   │   │   ├── lambda_function.py          # Main handler for data extraction
│   │   │   └── s3_utils.py                 # S3 file operations
│   │   ├── transform/
│   │   │   ├── Dockerfile                  # Container for lambda-transform
│   │   │   ├── requirements.txt            # Transformers, torch, mlflow
│   │   │   ├── lambda_function.py          # Sentiment analysis handler
│   │   │   ├── model_loader.py             # MLflow model loading utilities
│   │   │   └── sentiment_analyzer.py       # Sentiment classification logic
│   │   ├── monitor/
│   │   │   ├── Dockerfile                  # Container for lambda-monitor
│   │   │   ├── requirements.txt            # Evidently, boto3, psycopg2
│   │   │   ├── lambda_function.py          # Drift monitoring handler
│   │   │   └── drift_detector.py           # Evidently drift detection
│   │   └── alert/
│   │       ├── Dockerfile                  # Container for lambda-alert
│   │       ├── requirements.txt            # Boto3, sns
│   │       ├── lambda_function.py          # Alert/retraining handler
│   │       └── notification_sender.py      # SNS notification logic
│   ├── shared/
│   │   ├── __init__.py
│   │   ├── config.py                       # Shared configuration constants
│   │   ├── dynamodb_client.py              # DynamoDB state management
│   │   └── logger.py                       # Structured logging setup
│   ├── stepfunctions/
│   │   └── sentiment_pipeline.json         # Step Functions state machine definition
│   └── training/
│       ├── train_model.py                  # Model training script with MLflow
│       ├── data_prep.py                    # TikTok comment preprocessing
│       ├── feature_engineering.py          # Text feature extraction
│       └── evaluate_model.py               # Model evaluation metrics
├── infrastructure/
│   ├── terraform/
│   │   ├── main.tf                         # Main Terraform configuration
│   │   ├── variables.tf                    # Input variables
│   │   ├── outputs.tf                      # Output values
│   │   ├── modules/
│   │   │   ├── lambda/
│   │   │   │   ├── main.tf                 # Lambda function resources
│   │   │   │   ├── iam.tf                  # IAM roles and policies
│   │   │   │   └── variables.tf            # Lambda module variables
│   │   │   ├── stepfunctions/
│   │   │   │   ├── main.tf                 # Step Functions resources
│   │   │   │   ├── iam.tf                  # Step Functions IAM
│   │   │   │   └── variables.tf            # Step Functions variables
│   │   │   ├── eventbridge/
│   │   │   │   ├── main.tf                 # EventBridge rules and targets
│   │   │   │   └── variables.tf            # EventBridge variables
│   │   │   ├── s3/
│   │   │   │   ├── main.tf                 # S3 buckets for data/models
│   │   │   │   └── variables.tf            # S3 variables
│   │   │   ├── rds/
│   │   │   │   ├── main.tf                 # PostgreSQL RDS instance
│   │   │   │   ├── security_groups.tf      # Database security groups
│   │   │   │   └── variables.tf            # RDS variables
│   │   │   ├── dynamodb/
│   │   │   │   ├── main.tf                 # DynamoDB workflow state table
│   │   │   │   └── variables.tf            # DynamoDB variables
│   │   │   └── sns/
│   │   │       ├── main.tf                 # SNS topics for notifications
│   │       └── variables.tf            # SNS variables
│   │   └── environments/
│   │       ├── dev.tfvars                  # Development environment config
│   │       ├── staging.tfvars              # Staging environment config
│   │       └── prod.tfvars                 # Production environment config
├── data/
│   ├── raw/                                # Raw TikTok comment files (YYYY_MM_DD_HH.csv)
│   ├── processed/                          # Processed hourly sentiment data
│   └── reference/                          # Reference data for drift detection
├── models/
│   └── artifacts/                          # Local model artifacts for development
├── monitoring/
│   ├── dashboards/
│   │   ├── sentiment_drift.json            # CloudWatch dashboard for sentiment drift
│   │   └── model_performance.json          # Model performance metrics dashboard
│   └── alerts/
│       └── drift_alerts.yml                # CloudWatch alerting rules
├── tests/
│   ├── unit/
│   │   ├── test_extract.py                 # Unit tests for extraction lambda
│   │   ├── test_transform.py               # Unit tests for sentiment analysis
│   │   ├── test_monitor.py                 # Unit tests for drift monitoring
│   │   └── test_alert.py                   # Unit tests for alerting
│   ├── integration/
│   │   ├── docker-compose.yml              # Local test environment (PostgreSQL, MLflow)
│   │   ├── test_pipeline.py                # End-to-end pipeline tests
│   │   └── sample_data/                    # Test TikTok comment samples
│   └── load/
│       └── test_performance.py             # Load testing for Lambda functions
├── docker/
│   └── mlflow/
│       └── docker-compose.yml              # Local MLflow server
├── scripts/
│   ├── deploy.sh                           # Deployment script
│   ├── build_containers.sh                 # Build all Lambda containers
│   ├── setup_local_env.sh                  # Setup local development environment
│   ├── data_ingestion.py                   # Script to ingest sample TikTok data
│   └── teardown.sh                         # Terraform destroy wrapper script
├── docs/
│   ├── architecture.md                     # System architecture documentation
│   ├── deployment.md                       # Deployment instructions
│   └── api.md                              # API documentation
├── .gitignore                              # Git ignore patterns
├── .pre-commit-config.yaml                 # Pre-commit hooks configuration
├── Makefile                                # Build and deployment targets
├── README.md                               # Project overview and setup
├── requirements-dev.txt                    # Development dependencies
├── pyproject.toml                          # Python project configuration
└── sam-template.yaml                       # SAM template for local Lambda testing (using real AWS)

================================================================================
## STEP FUNCTIONS STATE MACHINE DEFINITION
================================================================================

{
  "Comment": "TikTok Sentiment Analysis Hourly Pipeline",
  "StartAt": "ExtractComments",
  "States": {
    "ExtractComments": {
      "Type": "Task",
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "${LambdaExtractArn}",
        "Payload": {
          "execution_id.$": "$$.Execution.Name",
          "timestamp.$": "$$.State.EnteredTime"
        }
      },
      "ResultPath": "$.extract_result",
      "Next": "TransformComments",
      "Retry": [
        {
          "ErrorEquals": ["States.TaskFailed"],
          "IntervalSeconds": 30,
          "MaxAttempts": 3,
          "BackoffRate": 2.0
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleExtractFailure",
          "ResultPath": "$.error"
        }
      ]
    },
    "TransformComments": {
      "Type": "Task", 
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "${LambdaTransformArn}",
        "Payload": {
          "execution_id.$": "$$.Execution.Name",
          "input_data.$": "$.extract_result.Payload"
        }
      },
      "ResultPath": "$.transform_result",
      "Next": "MonitorDrift",
      "Retry": [
        {
          "ErrorEquals": ["States.TaskFailed"],
          "IntervalSeconds": 30,
          "MaxAttempts": 2,
          "BackoffRate": 2.0
        }
      ]
    },
    "MonitorDrift": {
      "Type": "Task",
      "Resource": "arn:aws:states:::lambda:invoke", 
      "Parameters": {
        "FunctionName": "${LambdaMonitorArn}",
        "Payload": {
          "execution_id.$": "$$.Execution.Name",
          "predictions.$": "$.transform_result.Payload.predictions",
          "model_version.$": "$.transform_result.Payload.model_version"
        }
      },
      "ResultPath": "$.monitor_result",
      "Next": "CheckDriftThreshold"
    },
    "CheckDriftThreshold": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.monitor_result.Payload.drift_detected",
          "BooleanEquals": true,
          "Next": "SendAlert"
        }
      ],
      "Default": "PipelineSuccess"
    },
    "SendAlert": {
      "Type": "Task",
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "${LambdaAlertArn}",
        "Payload": {
          "execution_id.$": "$$.Execution.Name",
          "drift_score.$": "$.monitor_result.Payload.drift_score",
          "alert_type": "drift_detected"
        }
      },
      "Next": "PipelineSuccess"
    },
    "HandleExtractFailure": {
      "Type": "Task",
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "${LambdaAlertArn}",
        "Payload": {
          "execution_id.$": "$$.Execution.Name",
          "error.$": "$.error",
          "alert_type": "pipeline_failure"
        }
      },
      "Next": "PipelineFailure"
    },
    "PipelineSuccess": {
      "Type": "Pass",
      "Result": "Pipeline completed successfully",
      "End": true
    },
    "PipelineFailure": {
      "Type": "Fail",
      "Cause": "Pipeline execution failed"
    }
  }
}

================================================================================
## KEY IMPLEMENTATION NOTES
================================================================================

### Lambda Functions:
- **lambda-extract**: Pull hourly TikTok comment files from S3, validate schema
- **lambda-transform**: Run sentiment analysis using MLflow model, save results to RDS
- **lambda-monitor**: Compare current vs reference sentiment distribution using Evidently
- **lambda-alert**: Send SNS notifications for drift or failures, trigger retraining

### Step Functions:
- Orchestrates 4 Lambda functions with error handling and retries
- Conditional logic for drift detection threshold
- State persistence in DynamoDB for workflow tracking

### Infrastructure as Code:
- Terraform modules for all AWS resources
- Environment-specific configurations (dev/staging/prod)
- IAM roles with least privilege access

### Model Management:
- MLflow for experiment tracking and model registry
- Model versioning with semantic versioning
- Automated model promotion pipeline

### Monitoring & Alerting:
- Evidently for sentiment drift detection
- Custom CloudWatch metrics for pipeline health
- SNS notifications for alerts and failures

================================================================================
## REUSABLE PATTERNS FROM COURSE MODULES
================================================================================

### From Module 1 (Data Ingestion):
- **Pattern**: PostgreSQL connection with error handling
- **Reuse**: RDS PostgreSQL for storing sentiment results
- **Code**: Database connection patterns in lambda-transform

### From Module 2 (MLflow):
- **Pattern**: Model loading and prediction pipeline
- **Reuse**: MLflow model registry for sentiment models
- **Code**: model_loader.py uses MLflow client patterns

### From Module 3 (Prefect Workflow):
- **Pattern**: Task-based pipeline with retries and error handling
- **Reuse**: Step Functions with similar retry/error patterns
- **Code**: State machine definition mirrors Prefect flow structure

### From Module 4 (Lambda Deployment):
- **Pattern**: Containerized Lambda with model loading
- **Reuse**: All 4 Lambda functions use container pattern
- **Code**: Dockerfile patterns for each Lambda function

### From Module 5 (Evidently Monitoring):
- **Pattern**: Drift detection with reference data comparison
- **Reuse**: lambda-monitor uses identical Evidently patterns
- **Code**: drift_detector.py replicates evidently_metrics_calculation.py

### From Module 6 (Best Practices):
- **Pattern**: Terraform modular infrastructure
- **Reuse**: Complete Terraform structure with modules
- **Code**: terraform/ directory mirrors module_6 structure
- **Pattern**: CI/CD with tests and quality checks
- **Reuse**: .github/workflows/ for automated deployments
- **Pattern**: Makefile for build automation
- **Reuse**: Makefile with test, build, deploy targets

================================================================================
## AWS RESOURCES REQUIRED FOR TERRAFORM
================================================================================

### Networking:
- VPC with public/private subnets
- Security groups for RDS and Lambda
- NAT Gateway for Lambda internet access

### Compute:
- 4 Lambda functions (containerized)
- ECR repositories for Lambda containers
- Step Functions state machine

### Storage:
- S3 buckets: raw data, processed data, model artifacts
- RDS PostgreSQL instance for sentiment results
- DynamoDB table for workflow state tracking

### Integration:
- EventBridge rule for hourly scheduling
- SNS topics for notifications
- CloudWatch log groups for each Lambda

### Security:
- IAM roles for Lambda execution
- IAM role for Step Functions
- IAM policies with least privilege access
- VPC endpoints for S3/DynamoDB access

================================================================================
## ENVIRONMENT VARIABLES NEEDED  
================================================================================

### Global:
- PROJECT_NAME="tiktok-sentiment-analysis"
- ENVIRONMENT="dev|staging|prod"
- AWS_REGION="us-east-1"

### Lambda Extract:
- S3_RAW_BUCKET="tiktok-comments-raw-${env}"
- S3_PROCESSED_BUCKET="tiktok-comments-processed-${env}"

### Lambda Transform:
- MLFLOW_TRACKING_URI="https://mlflow.${env}.yourdomain.com"
- MODEL_NAME="tiktok-sentiment-model"
- MODEL_STAGE="Production"
- RDS_HOST="${rds_endpoint}"
- RDS_DATABASE="sentiment_db"
- RDS_USERNAME="sentiment_user"
- RDS_PASSWORD="${rds_password}"

### Lambda Monitor:
- REFERENCE_DATA_S3_PATH="s3://reference-data/sentiment_reference.parquet"
- DRIFT_THRESHOLD="0.1"
- EVIDENTLY_WORKSPACE_ID="${workspace_id}"

### Lambda Alert:
- SNS_TOPIC_ARN="${sns_topic_arn}"
- SLACK_WEBHOOK_URL="${slack_webhook}"
- RETRAINING_TRIGGER_TOPIC="${retraining_topic}"

### Step Functions:
- DYNAMODB_STATE_TABLE="workflow-state-${env}"
- MAX_RETRIES="3"
- RETRY_INTERVAL_SECONDS="30"

================================================================================
## SAM TEMPLATE FOR LOCAL LAMBDA TESTING
================================================================================

AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: TikTok Sentiment Analysis Local Testing

Globals:
  Function:
    Timeout: 300
    MemorySize: 512
    Runtime: python3.9
    Environment:
      Variables:
        PYTHONPATH: /var/task
        LOG_LEVEL: INFO

Resources:
  LambdaExtract:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: tiktok-extract-local
      CodeUri: src/lambdas/extract/
      Handler: lambda_function.lambda_handler
      Environment:
        Variables:
          S3_RAW_BUCKET: local-raw-bucket
          S3_PROCESSED_BUCKET: local-processed-bucket

  LambdaTransform:
    Type: AWS::Serverless::Function  
    Properties:
      FunctionName: tiktok-transform-local
      CodeUri: src/lambdas/transform/
      Handler: lambda_function.lambda_handler
      Environment:
        Variables:
          MLFLOW_TRACKING_URI: http://localhost:5000
          MODEL_NAME: sentiment-model-local
          RDS_HOST: localhost
          RDS_DATABASE: sentiment_test

  LocalStack:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: localstack-setup
      CodeUri: tests/integration/
      Handler: setup_localstack.handler

================================================================================
## EVALUATION CRITERIA COVERAGE
================================================================================

✅ **Cloud Deployment with IaC (4 points)**:
   - Complete Terraform infrastructure
   - Multi-environment support (dev/staging/prod)
   - Modular resource organization

✅ **Experiment Tracking and Model Registry (4 points)**:
   - MLflow integration for model management
   - Model versioning and promotion pipeline
   - Automated model loading in Lambda functions

✅ **Workflow Orchestration (4 points)**:
   - Step Functions state machine with conditional logic
   - EventBridge hourly scheduling
   - Error handling and retry mechanisms

✅ **Containerized Deployment (4 points)**:
   - All Lambda functions use container images
   - ECR repositories for container management
   - Docker build pipeline in CI/CD

✅ **Model Monitoring with Conditional Workflows (4 points)**:
   - Evidently drift detection
   - Conditional alerting based on thresholds
   - Automated retraining triggers

✅ **Best Practices (Bonus)**:
   - Unit/integration tests
   - Pre-commit hooks and linting
   - Makefile automation
   - CI/CD pipelines
   - Documentation

================================================================================
## NEXT STEPS FOR IMPLEMENTATION
================================================================================

1. **Initialize Project**: Create directory structure and basic files
2. **Setup MLflow**: Configure model training and registry
3. **Implement Lambda Functions**: Start with extract, then transform
4. **Create Terraform Modules**: Begin with core infrastructure
5. **Setup Step Functions**: Define and test state machine
6. **Add Monitoring**: Implement Evidently drift detection
7. **Configure CI/CD**: Setup GitHub Actions workflows
8. **Add Tests**: Unit, integration, and load tests
9. **Deploy to Staging**: Test full pipeline end-to-end
10. **Production Deployment**: Final deployment with monitoring

This structure provides a production-ready MLOps pipeline that demonstrates
all required competencies while following best practices from the course.